{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210210_customModel",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkAgbVUNfu/FWzLpoBc4UX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skotschi/KI-Kreativit-t/blob/main/210210_customModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91uNnboKnPEO",
        "outputId": "26a7917a-1495-4b41-ebe1-89880a4ac3a1"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 7.7MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE1UceEbnUnM"
      },
      "source": [
        "# Datensatz importieren. Beispielhaft wird ein Satz an Pegida Kommentaren genutzt.\n",
        "# Ursprüngliche Quelle der Daten ist https://github.com/adbar/German-NLP\n",
        "file_URL = 'https://raw.githubusercontent.com/skotschi/KI-Kreativit-t/main/pegida_korpus.txt'\n",
        "\n",
        "from requests import get\n",
        "with open('pegida_korpus.txt', 'wb') as big_f:\n",
        "    response = get(file_URL, )\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        big_f.write(response.content)\n",
        "    else:\n",
        "        print(\"Unable to get the file: {}\".format(response.reason))\n",
        "\n",
        "with open (\"pegida_korpus.txt\", \"r\") as myfile:\n",
        "  data=myfile.readlines()\n",
        "\n",
        "# Eingegebene Daten bereinigen\n",
        "# Regex zum Filtern der Einträge\n",
        "import re\n",
        "\n",
        "# Die einzelnen Kommentare sind mit 47 # Zeichen voneinander getrennt\n",
        "rule1 = re.compile(\"#{47}\")\n",
        "# Datum der jeweiligen Kommentare raus filter. Die Zeile beginnt immer mit Jahreszahl-Monat JJJJ-MM\n",
        "rule2 = re.compile(\"\\d{4}-\\d{2}\")\n",
        "\n",
        "# Ich weiß nicht, ob man die beiden nicht auch kombinieren kann. Also in zwei Zeilen.\n",
        "filtered_data = [i for i in data if not rule1.match(i)]\n",
        "filtered_data = [i for i in filtered_data if not rule2.match(i)]\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbsmNGpynoqH"
      },
      "source": [
        "# For the user's convenience `tokenizers` provides some very high-level classes encapsulating\n",
        "# the overall pipeline for various well-known tokenization algorithm. \n",
        "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Then we enable lower-casing and unicode-normalization\n",
        "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
        "# executed in order.\n",
        "tokenizer.normalizer = Sequence([\n",
        "    NFKC(),\n",
        "    Lowercase()\n",
        "])\n",
        "\n",
        "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "!touch filtered_pegida.txt\n",
        "\n",
        "with open('filtered_pegida.txt', 'w') as f:\n",
        "    f.writelines(filtered_data)\n",
        "f.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLIaxDKtnveD",
        "outputId": "f548aac6-a416-475c-a21f-6a89663873ec"
      },
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
        "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
        "tokenizer.train(files=[\"filtered_pegida.txt\"], trainer=trainer)\n",
        "\n",
        "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trained vocab size: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAi71uamn2vB",
        "outputId": "10e8b86d-6d1a-41b4-824f-486cdee58592"
      },
      "source": [
        "# You will see the generated files in the output.\n",
        "!mkdir pegibert\n",
        "tokenizer.model.save('./pegibert')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./pegibert/vocab.json', './pegibert/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m-9ccJloF2f",
        "outputId": "d2520cc4-b22a-4295-f3d8-aed7d837e07c"
      },
      "source": [
        "# Let's tokenizer a simple input\n",
        "tokenizer.model = BPE('./pegibert/vocab.json', './pegibert/merges.txt')\n",
        "encoding = tokenizer.encode(\"Ich wiederhole, aber das ist die Wahrheit. Auch Vollverschleierung ist ein schleichender Prozess. Der Weg zur Dunkelheit passiert nicht von heute auf Morgen. Aber am Ende steht immer die Finsternis.\")\n",
        "\n",
        "print(\"Encoded string: {}\".format(encoding.tokens))\n",
        "\n",
        "decoded = tokenizer.decode(encoding.ids)\n",
        "print(\"Decoded string: {}\".format(decoded))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded string: ['Ġich', 'Ġwiederhole', ',', 'Ġaber', 'Ġdas', 'Ġist', 'Ġdie', 'Ġwahrheit', '.', 'Ġauch', 'Ġvoll', 'verschleier', 'ung', 'Ġist', 'Ġein', 'Ġschleichen', 'der', 'Ġprozess', '.', 'Ġder', 'Ġweg', 'Ġzur', 'Ġdunkelheit', 'Ġpassiert', 'Ġnicht', 'Ġvon', 'Ġheute', 'Ġauf', 'Ġmorgen', '.', 'Ġaber', 'Ġam', 'Ġende', 'Ġsteht', 'Ġimmer', 'Ġdie', 'Ġf', 'inst', 'ern', 'is', '.']\n",
            "Decoded string:  ich wiederhole, aber das ist die wahrheit. auch vollverschleierung ist ein schleichender prozess. der weg zur dunkelheit passiert nicht von heute auf morgen. aber am ende steht immer die finsternis.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Deprecated in 0.9.0: BPE.__init__ will not create from files anymore, try `BPE.from_file` instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP0Fswg_ofw4",
        "outputId": "6d9d0497-0ade-4fd0-e19c-4d8b5acdfa7f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 30.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=ef3c90ab8c574017b020097fca3b0f3c0219d191a5c5e90ff8666a569cdf6161\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 transformers-4.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE84pwhCoOO0",
        "outputId": "cf6e580b-6bd0-4545-8643-1ba074967f70"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 13 21:03:44 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd3NVkDiobyU",
        "outputId": "17e96214-4609-473a-e962-d90322fccd6e"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I47XJnmCpsLK"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr-iwXX3pu3R",
        "outputId": "231c0cff-3d22-4bb5-fa89-32cee41c1e53"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 119592\n",
            "drwxr-xr-x 1 root root     4096 Feb 13 21:03 .\n",
            "drwxr-xr-x 1 root root     4096 Feb 13 20:49 ..\n",
            "drwxr-xr-x 1 root root     4096 Feb 10 14:40 .config\n",
            "-rw-r--r-- 1 root root 50916549 Feb 13 21:02 filtered_pegida.txt\n",
            "drwxr-xr-x 2 root root     4096 Feb 13 21:03 pegibert\n",
            "-rw-r--r-- 1 root root 71518149 Feb 13 21:02 pegida_korpus.txt\n",
            "drwxr-xr-x 1 root root     4096 Feb 10 14:40 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrGPpkf8pyk6",
        "outputId": "458aa434-a078-4e17-9218-a50c7da5b251"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"./pegibert\", max_len=512)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA_-jx-pqM-L"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpMCk_G8qCeq",
        "outputId": "ae607351-893a-4c79-d3cb-83d6c7da3887"
      },
      "source": [
        "model.num_parameters()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83504416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVnGlcVPqE5G",
        "outputId": "06bbf1bf-98d9-40d6-ec77-d3857b85cde4"
      },
      "source": [
        "%%time\n",
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./pegida_korpus.txt\",\n",
        "    block_size=128,\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 21s, sys: 2.95 s, total: 1min 23s\n",
            "Wall time: 55.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LcloT7XqXP9"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1uKQzgRqlhE"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pegibert\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=64,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "#    prediction_loss_only=True,\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "T2hzlbm1qnng",
        "outputId": "678d387d-44d0-4129-f166-73171816c07d"
      },
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='434' max='14651' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  434/14651 09:20 < 5:07:35, 0.77 it/s, Epoch 0.03/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7uueDNtq0DA"
      },
      "source": [
        "trainer.save_model(\"./pegibert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybYpn4rhvO2U"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./pegibert\",\n",
        "    tokenizer=\"./pegibert\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9SvqXJhvRJe"
      },
      "source": [
        "fill_mask(\"In Dresden <mask>.\")\n",
        "print(fill_mask(f\"Der {fill_mask.tokenizer.mask_token} ist eine große Gefahr für Deutschland.\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7ny5OrxyD6i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}